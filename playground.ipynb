{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO per Valentina\n",
    "\n",
    "- go through the files she sent on Teams\n",
    "- think once again whats how to improve actions identification - still does mistakes\n",
    "- Make possible to query two files at once\n",
    "- get some information on where the fuck does she want to publish?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm import load_models\n",
    "from dotenv import load_dotenv\n",
    "from questioning import load_qdf, ask_qdfs, pdfs_to_documents, documents_to_vector_store, query_action_detail, query_action_SMART, query_action_list, ask_RAG, filter_action_pages\n",
    "\n",
    "from logging_config import logging_config\n",
    "logger = logging_config(f'GPT@JRC','logger.log')\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from prompts import PROMPTS\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "config = {\n",
    "    'Torino':{\n",
    "        'pdfs': [\n",
    "            {'file':'Torino.pdf','action_page_limit': (143, 191)}\n",
    "        ],\n",
    "    },\n",
    "    'Zaragoza':{\n",
    "        'pdfs': [\n",
    "            {'file':'Zaragoza.pdf','action_page_limit': (0, 200)}\n",
    "        ],\n",
    "    },\n",
    "    'Bologna':{\n",
    "        'pdfs': [\n",
    "            {'file':'Bologna.pdf','action_page_limit': (128, 194)}\n",
    "        ],\n",
    "    },\n",
    "    'Izmir':{\n",
    "        'pdfs': [\n",
    "            {'file':'Izmir.pdf','action_page_limit': (58, 142)}\n",
    "        ],\n",
    "    },\n",
    "    'Lisbon':{\n",
    "        'pdfs': [\n",
    "            {'file':'Lisbon_1.pdf','action_page_limit': (0, 200)},\n",
    "            {'file':'Lisbon_2.pdf','action_page_limit': (0, 200)}\n",
    "        ],\n",
    "    },\n",
    "    'Liepaja':{\n",
    "        'pdfs': [\n",
    "            {'file':'Liepaja_1.pdf','action_page_limit': (60, 100)},\n",
    "            #{'file':'Liepaja_2.pdf','action_page_limit': (0, 200)} # seem to be old version\n",
    "        ],\n",
    "    },\n",
    "    'Valencia':{\n",
    "        'pdfs': [\n",
    "            {'file':'Valencia_1.pdf','action_page_limit': (0, 200)},\n",
    "            {'file':'Valencia_2.pdf','action_page_limit': (0, 200)}\n",
    "        ],\n",
    "    },\n",
    "    'Porto':{\n",
    "        'pdfs': [\n",
    "            {'file':'Porto_1.pdf','action_page_limit': (0, 200)},\n",
    "            {'file':'Porto_2.pdf','action_page_limit': (0, 200)}\n",
    "        ],\n",
    "    },\n",
    "    'Lappeenranta':{\n",
    "        'pdfs': [\n",
    "            {'file':'Lappeenranta.pdf','action_page_limit': (0, 200)}\n",
    "            \n",
    "        ],\n",
    "    },\n",
    "    'Firenze':{\n",
    "        'pdfs': [\n",
    "            {'file':'Firenze.pdf','action_page_limit': (0, 200)},\n",
    "        ],\n",
    "    },\n",
    "    'Ioannina':{\n",
    "        'pdfs': [\n",
    "            {'file':'Ioannina.pdf','action_page_limit': (0, 200)}\n",
    "        ]\n",
    "    },\n",
    "    'Kalamata':{\n",
    "        'pdfs': [\n",
    "            {'file':'Kalamata.pdf','action_page_limit': (0, 200)}\n",
    "        ]\n",
    "    },\n",
    "    'Tampere':{\n",
    "        'pdfs': [\n",
    "            {'file':'Tampere.pdf','action_page_limit': (0, 200)}\n",
    "        ],\n",
    "    },\n",
    "    'Espoo':{\n",
    "        'pdfs': [\n",
    "            {'file':'Espoo.pdf','action_page_limit': (0, 200)}\n",
    "        ],\n",
    "    },\n",
    "    'Guimaraes':{\n",
    "        'pdfs': [\n",
    "            {'file':'Guimaraes.pdf','action_page_limit': (0, 200)}\n",
    "        ]\n",
    "    },\n",
    "    'Lahti':{\n",
    "        'pdfs': [\n",
    "            {'file':'Lahti.pdf','action_page_limit': (0, 200)}\n",
    "        ],\n",
    "    },\n",
    "    'Kozani':{\n",
    "        'pdfs': [\n",
    "            {'file':'Kozani.pdf','action_page_limit': (0, 200)}\n",
    "        ],\n",
    "    },\n",
    "    'Pecs':{\n",
    "        'pdfs': [\n",
    "            {'file':'Pecs.pdf','action_page_limit': (0, 200)}\n",
    "        ],\n",
    "    },\n",
    "    'Sevilla':{\n",
    "        'pdfs': [\n",
    "            {'file':'Sevilla.pdf','action_page_limit': (0, 200)}\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "load_dotenv()\n",
    "llm, emb = load_models(service = 'DeepInfra')\n",
    "qdf = load_qdf() # questions from JSON\n",
    "pdf_prefix='pdf_input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-15 13:01:00,979 - GPT@JRC - INFO - Starting PDFs of Lappeenranta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Questions: 100%|██████████| 58/58 [09:03<00:00,  9.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-15 13:11:17,299 - GPT@JRC - INFO - Action list...\n",
      "2025-01-15 13:12:46,674 - GPT@JRC - INFO - Action details...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action details: 100%|██████████| 22/22 [06:56<00:00, 18.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-15 13:19:42,869 - GPT@JRC - INFO - Action SMARTs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Action SMART:  59%|█████▉    | 13/22 [01:42<01:05,  7.33s/it]"
     ]
    }
   ],
   "source": [
    "def pdf_to_excel(city, excel_output, llm=llm, emb=emb, pdf_prefix='pdf_input/',config=config):\n",
    "    files = [f'{pdf_prefix}{pdf[\"file\"]}' for pdf in config[city]['pdfs']]\n",
    "\n",
    "    logger.info(f'Starting PDFs of {city}')    \n",
    "    logger.debug('Reading the documents..')\n",
    "    documents, splits = pdfs_to_documents(files)\n",
    "    \n",
    "    logger.debug('Converting to vector store ...')\n",
    "    vector_store = documents_to_vector_store(documents, emb)\n",
    "\n",
    "    with pd.ExcelWriter(excel_output) as xlsx:\n",
    "\n",
    "        qs_responses = ask_qdfs(\n",
    "            qdf = qdf,\n",
    "            llm = llm, \n",
    "            vector_store = vector_store,\n",
    "            city = city,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        qs_responses.to_excel(xlsx,sheet_name='standard_qs')\n",
    "\n",
    "        logger.debug('Barriers...')\n",
    "\n",
    "        class Barrier(BaseModel):\n",
    "            title: str\n",
    "            brief_description: str\n",
    "            page_reference: str\n",
    "            explanation: str\n",
    "            category: List[Literal[\"Leadership\", \"Financial\", \"Regulatory\", \"Operational\", \"Organisational\", \"Partnerships\", \"Social\", \"Environmental\", \"Safety and Security\", \"Other\"]]\n",
    "        \n",
    "        class BarrierList(BaseModel):\n",
    "            barriers: List[Barrier]\n",
    "\n",
    "\n",
    "        response = ask_RAG(\n",
    "            embed_query = 'List all barriers to accelerating emission reduction in view of the 2030 climate-neutrality goal that are explicitly described in the plan',\n",
    "            vector_store = vector_store,\n",
    "            response_model=BarrierList,\n",
    "            llm = llm,\n",
    "            logger = logger,\n",
    "            template_key = 'barriers_prompt',\n",
    "            template_kwargs={},\n",
    "            city = city\n",
    "        )\n",
    "        bars = pd.DataFrame([b for b in response['barriers']])\n",
    "        bars.to_excel(xlsx, sheet_name = 'barriers')\n",
    "\n",
    "        logger.debug('Participatory processes...')\n",
    "\n",
    "        class Stakeholder(BaseModel):\n",
    "            stakeholder: str\n",
    "            role: str\n",
    "\n",
    "        class ParticipatoryProcess(BaseModel):\n",
    "            title: str\n",
    "            brief_description: str = Field(description=\"Description in English\")\n",
    "            page_reference: str\n",
    "            explanation: str\n",
    "            relevant_stakeholders: List[Stakeholder]\n",
    "            implementation_barriers: Optional[List[str]] = []\n",
    "            barriers_solution: Optional[List[str]] = []\n",
    "            citizen_contribution_clearly_defined: bool\n",
    "\n",
    "        class ParticipatoryProcesses(BaseModel):\n",
    "            participatory_processes: List[ParticipatoryProcess]\n",
    "\n",
    "\n",
    "        response = ask_RAG(\n",
    "            embed_query='''\n",
    "            - List all participatory process described in the SECAP plan.\n",
    "            - Describe how citizens' contribute to specific priorities in the plan\n",
    "            - Inclusion of participatory processes and engagement of stakeholder\n",
    "            - Barriers to /challenges for citizen engagement and solutions/opportunities to remove it\n",
    "            ''',\n",
    "            template_key='participatory_processes',\n",
    "            template_kwargs={},\n",
    "            response_model=ParticipatoryProcesses,\n",
    "            vector_store=vector_store,\n",
    "            llm=llm,\n",
    "            logger=logger,\n",
    "            city=city\n",
    "        )\n",
    "\n",
    "        participatory_processes = pd.DataFrame([pp for pp in response['participatory_processes']])\n",
    "        participatory_processes.to_excel(xlsx, sheet_name='participatory_processes')\n",
    "\n",
    "        logger.info(f'Action list...')\n",
    "\n",
    "        action_list = query_action_list(\n",
    "            llm=llm, \n",
    "            documents=filter_action_pages(splits, config[city]),\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        logger.info(f'Action details...')\n",
    "        ads = [query_action_detail(\n",
    "                action=action, \n",
    "                llm=llm, \n",
    "                vector_store=vector_store, \n",
    "                logger=logger,\n",
    "                city = city\n",
    "            ) \n",
    "            for action in tqdm(action_list.to_dict(orient='records'),desc='Action details')]\n",
    "        action_details = pd.json_normalize([ad for ad in ads if ad is not None])\n",
    "\n",
    "        action_details.to_excel(xlsx, sheet_name='Actions')\n",
    "        logger.info(f'Action SMARTs...')\n",
    "\n",
    "        smarts = [query_action_SMART(\n",
    "                action=action,\n",
    "                llm=llm,\n",
    "                vector_store=vector_store, \n",
    "                logger=logger,\n",
    "                city = city\n",
    "            ) for action in tqdm(action_list.to_dict(orient='records'),desc='Action SMART')]\n",
    "        action_SMART = pd.json_normalize([sm for sm in smarts if sm is not None])\n",
    "\n",
    "        action_SMART.to_excel(xlsx, sheet_name='Actions_SMART')\n",
    "\n",
    "for city in list(config.keys())[-11:]:\n",
    "    pdf_to_excel(\n",
    "        city, \n",
    "        f'output/{city}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plgrnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
